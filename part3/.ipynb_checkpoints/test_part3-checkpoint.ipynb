{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "from time import time\n",
    "from itertools import product\n",
    "from scipy.stats import truncnorm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    from img_to_mat.img_to_mat import img_to_mat, mat_to_img\n",
    "except ImportError:\n",
    "    print('run the following from the img_to_mat directory and try again:')\n",
    "    print('python setup.py build_ext --inplace')\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2\n",
    "# Weight and bias\n",
    "\n",
    "def weight_variable(shape):\n",
    "    \"\"\"\n",
    "    Initialize weight matrices with truncated normal distribution with standard deviation of 0.1.\n",
    "\n",
    "    In general, shape = [filter_height, filter_width, in_channels, out_channels].\n",
    "    \"\"\"\n",
    "    init = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(init)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    \"\"\"\n",
    "    Initialize bias vectors with constant value of 0.1.\n",
    "\n",
    "    In general, shape = [out_channels].\n",
    "    \"\"\"\n",
    "    init = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(init)\n",
    "\n",
    "\n",
    "# Layers\n",
    "\n",
    "def conv2d(x, W):\n",
    "    \"\"\"2D convolution of x with filter W, unit stride.\"\"\"\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def maxpool2d(x):\n",
    "    \"\"\"2x2 max pooling layer with non-overlapping kernel stride.\"\"\"\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "def flatten(x):\n",
    "    size = 1\n",
    "    for dim in x.shape.as_list()[1:]:\n",
    "        size *= dim\n",
    "    return tf.reshape(x, [-1, size])\n",
    "\n",
    "def fullconn(x, W):\n",
    "    return tf.matmul(x, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN:\n",
    "    class Utils:\n",
    "        def pad(x, ksize, strides, padding='SAME'):\n",
    "            batch_size, in_height, in_width, in_channels = x.shape\n",
    "            k_batch_size, k_height, k_width, k_channels = ksize\n",
    "            if padding == 'SAME':\n",
    "                out_height = ceil(float(in_height) / float(strides[1]))\n",
    "                out_width = ceil(float(in_width) / float(strides[2]))\n",
    "                if (in_height % strides[1] == 0):\n",
    "                    pad_height = max(k_height - strides[1], 0)\n",
    "                else:\n",
    "                    pad_height = max(k_height - (in_height % strides[1]), 0)\n",
    "                if (in_width % strides[2] == 0):\n",
    "                    pad_width = max(k_width - strides[2], 0)\n",
    "                else:\n",
    "                    pad_width = max(k_width - (in_width % strides[2]), 0)\n",
    "                pad_top = pad_height // 2\n",
    "                pad_bottom = pad_height - pad_top\n",
    "                pad_left = pad_width // 2\n",
    "                pad_right = pad_width - pad_left\n",
    "            elif padding == 'VALID':\n",
    "                out_height = ceil(float(in_height - k_height + 1) / float(strides[1]))\n",
    "                out_width = ceil(float(in_width - k_width + 1) / float(strides[2]))\n",
    "                pad_top = 0\n",
    "                pad_bottom = 0\n",
    "                pad_left = 0\n",
    "                pad_right = 0\n",
    "            return pad_top, pad_bottom, pad_left, pad_right, out_height, out_width\n",
    "        \n",
    "        # def img_to_col(x, out_height, out_width, filter_height, filter_width, strides):\n",
    "        #     batch_size = x.shape[0]\n",
    "        #     in_channels = x.shape[3]\n",
    "        #     x_col = np.zeros((batch_size * out_height * out_width, filter_height * filter_width * in_channels))\n",
    "        #     for b in range(batch_size):\n",
    "        #         for i in range(out_height):\n",
    "        #             for j in range(out_width):\n",
    "        #                 row = b * out_height * out_width + i * out_width + j\n",
    "        #                 for p in range(filter_height):\n",
    "        #                     for q in range(filter_width):\n",
    "        #                         for r in range(in_channels):\n",
    "        #                             col = p * filter_width * in_channels + q * in_channels + r\n",
    "        #                             x_col[row, col] += x[b, strides[1] * i + p, strides[2] * j + q, r]\n",
    "        #     return x_col\n",
    "        \n",
    "        # def col_to_img(x_col, out_height, out_width, in_height, in_width, filter_height, filter_width, strides):\n",
    "        #     batch_size = x_col.shape[0] // (out_height * out_width)\n",
    "        #     in_channels = x_col.shape[1] // (filter_height * filter_width)\n",
    "        #     x = np.zeros((batch_size, in_height, in_width, in_channels))\n",
    "        #     for b in range(batch_size):\n",
    "        #         for i in range(out_height):\n",
    "        #             for j in range(out_width):\n",
    "        #                 row = b * out_height * out_width + i * out_width + j\n",
    "        #                 for p in range(filter_height):\n",
    "        #                     for q in range(filter_width):\n",
    "        #                         for r in range(in_channels):\n",
    "        #                             col = p * filter_width * in_channels + q * in_channels + r\n",
    "        #                             x[b, strides[1] * i + p, strides[2] * j + q, r] += x_col[row, col]\n",
    "        #     return x\n",
    "        \n",
    "        \n",
    "    class Variables:\n",
    "        def weight_variable(shape):\n",
    "            \"\"\"\n",
    "            Initialize weight matrices with truncated normal distribution with standard deviation of 0.1.\n",
    "\n",
    "            In general, shape = [filter_height, filter_width, in_channels, out_channels].\n",
    "            \"\"\"\n",
    "            stddev = 0.1\n",
    "            size = 1\n",
    "            for dim in shape:\n",
    "                size *= dim\n",
    "            return truncnorm.rvs(-2 * stddev, 2 * stddev, size=size).reshape(shape)\n",
    "\n",
    "        def bias_variable(shape):\n",
    "            \"\"\"\n",
    "            Initialize bias vectors with constant value of 0.1.\n",
    "\n",
    "            In general, shape = [out_channels].\n",
    "            \"\"\"\n",
    "            const = 0.1\n",
    "            return np.full(shape, const)\n",
    "    \n",
    "    \n",
    "    class Activations:\n",
    "        class relu:\n",
    "            def __init__(self, z):\n",
    "                \"\"\"Compute the rectified linear unit using z.\"\"\"\n",
    "                self.z = z\n",
    "                self.mask_forward = (z >= 0).astype(np.float32)\n",
    "                mask_zero = (z == 0).astype(np.float32)\n",
    "                self.mask_backward = self.mask_forward - 0.5 * mask_zero\n",
    "\n",
    "            def out(self):\n",
    "                self.out = self.z * self.mask_forward\n",
    "                return self.out\n",
    "\n",
    "            def grad_z(self, grad_out):\n",
    "                \"\"\"Compute the derivative of the rectified linear unit using z.\"\"\"\n",
    "                print(grad_out)\n",
    "                self.grad_z = grad_out * self.mask_backward\n",
    "                return self.grad_z\n",
    "\n",
    "\n",
    "        class softmax:\n",
    "            def __init__(self, z):\n",
    "                \"\"\"Compute the softmax of z.\"\"\"\n",
    "                self.z = z\n",
    "\n",
    "            def out(self):\n",
    "                z_reshaped = self.z.reshape((self.z.shape[0], -1))\n",
    "                z_max_reshaped = z_reshaped.max(axis = 1, keepdims = True)\n",
    "                e_z_reshaped = np.exp(z_reshaped - z_max_reshaped)\n",
    "                e_z_sum_reshaped = e_z_reshaped.sum(axis = 1, keepdims = True)\n",
    "                out_reshaped = e_z_reshaped / e_z_sum_reshaped\n",
    "                self.out = out_reshaped.reshape(self.z.shape)\n",
    "                return self.out\n",
    "            \n",
    "            def grad_z(self, grad_out):\n",
    "                \"\"\"Compute the derivative of the softmax of z.\"\"\"\n",
    "                # self.grad_z = (grad_out * self.out) - grad_out.dot(np.outer(self.out, self.out))\n",
    "                grad_out_reshaped = grad_out.reshape((grad_out.shape[0], -1))\n",
    "                out_reshaped = self.out.reshape((self.out.shape[0], -1))\n",
    "                inner = np.diagonal(grad_out_reshaped.dot(out_reshaped.T)).reshape((-1, 1))\n",
    "                self.grad_z = (grad_out_reshaped - inner).reshape(grad_out.shape) * self.out\n",
    "                return self.grad_z\n",
    "    \n",
    "    \n",
    "    class Layers:\n",
    "        class flatten:\n",
    "            def __init__(self, x):\n",
    "                \"\"\"Flatten tensor to shape [batch_size, x_height * x_width * in_channels].\"\"\"\n",
    "                self.x = x\n",
    "\n",
    "            def out(self):\n",
    "                self.out_val = self.x.reshape((self.x.shape[0], -1))\n",
    "                return self.out_val\n",
    "            \n",
    "            def grad_x(self, grad_out):\n",
    "                self.grad_x_val = grad_out.reshape(self.x.shape)\n",
    "                return self.grad_x_val\n",
    "            \n",
    "        \n",
    "        class fullconn:\n",
    "            def __init__(self, x, w):\n",
    "                \"\"\"Fully connected layer.\"\"\"\n",
    "                assert(x.shape[-1] == w.shape[0])\n",
    "                self.x = x\n",
    "                self.w = w\n",
    "                self.batch_size = x.shape[0]\n",
    "                self.in_channels = x.shape[1]\n",
    "                self.out_channels = w.shape[1]\n",
    "\n",
    "            def out(self):\n",
    "                self.out_val = self.x.dot(self.w)\n",
    "                return self.out_val\n",
    "\n",
    "            def grad_x(self, grad_out):\n",
    "                self.grad_x_val = grad_out.dot(self.w.T)\n",
    "                return self.grad_x_val\n",
    "\n",
    "            def grad_w(self, grad_out):\n",
    "                self.grad_w_val = self.x.T.dot(grad_out)\n",
    "                return self.grad_w_val\n",
    "            \n",
    "            \n",
    "        class conv2d:\n",
    "            def __init__(self, x, w, strides=[1, 1, 1, 1], padding='SAME'):\n",
    "                assert(x.shape[3] == w.shape[2])\n",
    "                self.x = x\n",
    "                self.w = w\n",
    "                self.strides = strides\n",
    "                self.batch_size = x.shape[0]\n",
    "                self.in_height = x.shape[1]\n",
    "                self.in_width = x.shape[2]\n",
    "                self.filter_height = w.shape[0]\n",
    "                self.filter_width = w.shape[1]\n",
    "                self.in_channels = w.shape[2]\n",
    "                self.out_channels = w.shape[3]\n",
    "                \n",
    "                ksize = (1, self.filter_height, self.filter_width, 1)\n",
    "                padding_out = NN.Utils.pad(x, ksize, strides, padding = padding)\n",
    "                self.pad_top = padding_out[0]\n",
    "                self.pad_bottom = padding_out[1]\n",
    "                self.pad_left = padding_out[2]\n",
    "                self.pad_right = padding_out[3]\n",
    "                self.out_height = padding_out[4]\n",
    "                self.out_width = padding_out[5]\n",
    "                self.x_pad = np.pad(x, [(0, 0), (self.pad_top, self.pad_bottom), (self.pad_left, self.pad_right), (0, 0)], mode='constant')\n",
    "            \n",
    "            def out_bkp(self):\n",
    "                self.out = np.empty((self.batch_size, self.out_height, self.out_width, self.out_channels))\n",
    "                for b in range(self.batch_size):\n",
    "                    for i in range(self.out_height):\n",
    "                        for j in range(self.out_width):\n",
    "                            for k in range(self.out_channels):\n",
    "                                self.out[b, i, j, k] = (self.x_pad[b, (self.strides[1] * i):(self.strides[1] * i + self.filter_height), (self.strides[2] * j):(self.strides[2] * j + self.filter_width), :] * self.w[::-1, ::-1, :, k]).sum(axis = (0, 1))\n",
    "                return self.out\n",
    "            \n",
    "            def out(self):\n",
    "                # x_col = NN.Utils.img_to_col(self.x_pad, self.out_height, self.out_width, self.filter_height, self.filter_width, self.strides)\n",
    "                x_mat = img_to_mat(self.x_pad, self.out_height, self.out_width, self.filter_height, self.filter_width, self.strides)\n",
    "#                 w_mat = self.w[::-1, ::-1].reshape((-1, self.out_channels))\n",
    "                w_mat = self.w.reshape((-1, self.out_channels))\n",
    "                self.fullconn = NN.Layers.fullconn(x_mat, w_mat)\n",
    "                out_mat = self.fullconn.out()\n",
    "                self.out_val = out_mat.reshape((self.batch_size, self.out_height, self.out_width, self.out_channels))\n",
    "                return self.out_val\n",
    "            \n",
    "            def grad_x(self, grad_out):\n",
    "                grad_out_reshaped = grad_out.reshape((-1, self.out_channels))\n",
    "                grad_x_mat = self.fullconn.grad_x(grad_out_reshaped)\n",
    "                # self.grad_x_val = NN.Utils.col_to_img(grad_x_col, self.out_height, self.out_width, self.x_pad.shape[1], self.x_pad.shape[2], self.filter_height, self.filter_width, self.strides)\n",
    "                self.grad_x_val = mat_to_img(grad_x_mat, self.out_height, self.out_width, self.x_pad.shape[1], self.x_pad.shape[2], self.filter_height, self.filter_width, self.strides)\n",
    "                if self.pad_top > 0:\n",
    "                    self.grad_x_val = self.grad_x_val[:, self.pad_top:, :, :]\n",
    "                if self.pad_bottom > 0:\n",
    "                    self.grad_x_val = self.grad_x_val[:, :-self.pad_bottom, :, :]\n",
    "                if self.pad_left > 0:\n",
    "                    self.grad_x_val = self.grad_x_val[:, :, self.pad_left:, :]\n",
    "                if self.pad_right > 0:\n",
    "                    self.grad_x_val = self.grad_x_val[:, :, :-self.pad_right, :]\n",
    "                return self.grad_x_val\n",
    "\n",
    "            def grad_w(self, grad_out):\n",
    "                grad_out_reshaped = grad_out.reshape((self.batch_size * self.out_height * self.out_width, self.out_channels))\n",
    "                grad_w_col = self.fullconn.grad_w(grad_out_reshaped)\n",
    "                self.grad_w_val = grad_w_col.reshape(self.w.shape)\n",
    "                return self.grad_w_val\n",
    "\n",
    "\n",
    "        class maxpool2d:\n",
    "            def __init__(self, x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME'):\n",
    "                self.x = x\n",
    "                self.ksize = ksize\n",
    "                self.strides = strides\n",
    "                self.batch_size = x.shape[0]\n",
    "                self.in_height = x.shape[1]\n",
    "                self.in_width = x.shape[2]\n",
    "                self.in_channels = x.shape[3]\n",
    "                self.out_channels = self.in_channels\n",
    "                padding_out = NN.Utils.pad(x, ksize, strides, padding = padding)\n",
    "                self.pad_top = padding_out[0]\n",
    "                self.pad_bottom = padding_out[1]\n",
    "                self.pad_left = padding_out[2]\n",
    "                self.pad_right = padding_out[3]\n",
    "                self.out_height = padding_out[4]\n",
    "                self.out_width = padding_out[5]\n",
    "                self.x_pad = np.pad(x, [(0, 0), (self.pad_top, self.pad_bottom), (self.pad_left, self.pad_right), (0, 0)], mode='constant')\n",
    "\n",
    "            def out_bkp(self):\n",
    "                self.out_val = np.empty((self.batch_size, self.out_height, self.out_width, self.out_channels))\n",
    "                self.pos = np.zeros((self.batch_size, self.out_height, self.out_width, self.out_channels, 4), dtype = int)\n",
    "                for b in range(self.batch_size):\n",
    "                    for i in range(self.out_height):\n",
    "                        stride_i = self.strides[1] * i\n",
    "                        for j in range(self.out_width):\n",
    "                            stride_j = self.strides[2] * j\n",
    "                            for k in range(self.out_channels):\n",
    "                                x_block = self.x_pad[b, (self.strides[1] * i):(self.strides[1] * i + self.ksize[1]), (self.strides[2] * j):(self.strides[2] * j + self.ksize[2]), k]\n",
    "                                max_val = x_block.max()\n",
    "                                self.out_val[b, i, j, k] = max_val\n",
    "                                max_inds = list(np.unravel_index(x_block.argmax(), x_block.shape))\n",
    "                                self.pos[b, i, j, k] = np.array([b, self.strides[1] * i + max_inds[0], self.strides[2] * j + max_inds[1], k])\n",
    "                return self.out_val\n",
    "            \n",
    "            def grad_x_bkp(self, grad_out):\n",
    "                self.grad_x_val = np.zeros(self.x_pad.shape)\n",
    "                for b in range(self.batch_size):\n",
    "                    for i in range(self.out_height):\n",
    "                        for j in range(self.out_width):\n",
    "                            for k in range(self.out_channels):\n",
    "                                self.grad_x_val[tuple(self.pos[b, i, j, k])] = grad_out[b, i, j, k]\n",
    "                return self.grad_x_val\n",
    "            \n",
    "            def out(self):\n",
    "                x_reshaped = self.x_pad.transpose((3, 0, 1, 2)).reshape((self.in_channels * self.batch_size, self.in_height, self.in_width, 1))\n",
    "                # x_col = NN.Utils.img_to_col(x_reshaped, self.out_height, self.out_width, self.ksize[1], self.ksize[2], self.strides)\n",
    "                x_mat = img_to_mat(x_reshaped, self.out_height, self.out_width, self.ksize[1], self.ksize[2], self.strides)\n",
    "                self.x_mat_maxcols = x_mat.argmax(axis = 1)\n",
    "                out_reshaped = x_mat[np.arange(x_mat.shape[0]), self.x_mat_maxcols]\n",
    "                self.out_val = out_reshaped.reshape((self.in_channels, self.batch_size, self.out_height, self.out_width)).transpose((1, 2, 3, 0))\n",
    "                return self.out_val\n",
    "            \n",
    "            def grad_x(self, grad_out):\n",
    "                grad_x_mat = np.zeros((self.in_channels * self.batch_size * self.out_height * self.out_width, self.ksize[1] * self.ksize[2]))\n",
    "                grad_x_mat[np.arange(grad_x_mat.shape[0]), self.x_mat_maxcols] = grad_out.transpose((3, 0, 1, 2)).flatten()\n",
    "                # grad_x_reshaped = NN.Utils.col_to_img(grad_x_mat, self.out_height, self.out_width, self.x_pad.shape[1], self.x_pad.shape[2], self.ksize[1], self.ksize[2], self.strides)\n",
    "                grad_x_reshaped = mat_to_img(grad_x_mat, self.out_height, self.out_width, self.x_pad.shape[1], self.x_pad.shape[2], self.ksize[1], self.ksize[2], self.strides)\n",
    "                if self.pad_top > 0:\n",
    "                    grad_x_reshaped = grad_x_reshaped[:, self.pad_top:, :, :]\n",
    "                if self.pad_bottom > 0:\n",
    "                    grad_x_reshaped = grad_x_reshaped[:, :-self.pad_bottom, :, :]\n",
    "                if self.pad_left > 0:\n",
    "                    grad_x_reshaped = grad_x_reshaped[:, :, self.pad_left:, :]\n",
    "                if self.pad_right > 0:\n",
    "                    grad_x_reshaped = grad_x_reshaped[:, :, :-self.pad_right, :]\n",
    "                self.grad_x_val = grad_x_reshaped.reshape((self.in_channels, self.batch_size, self.in_height, self.in_width)).transpose((1, 2, 3, 0))\n",
    "                return self.grad_x_val\n",
    "    \n",
    "    \n",
    "    class Cost:\n",
    "        class mean_squared_error:\n",
    "            def __init__(self, y_label, y_out):\n",
    "                self.y_label = y_label\n",
    "                self.y_out = y_out\n",
    "            \n",
    "            def out(self):\n",
    "                self.out_val = 0.5 * (self.y_label - self.y_out) ** 2\n",
    "                return self.out_val\n",
    "            \n",
    "            def grad_x(self):\n",
    "                self.grad_x_val = self.y_out - self.y_label\n",
    "                return self.grad_x_val\n",
    "\n",
    "\n",
    "        class cross_entropy:\n",
    "            def __init__(self, y_label, y_out):\n",
    "                self.y_label = y_label\n",
    "                self.y_out = y_out\n",
    "            \n",
    "            def out(self):\n",
    "                self.out_val = -np.sum(self.y_label * np.log(self.y_out), axis = 1)\n",
    "                return self.out_val\n",
    "            \n",
    "            def grad_x(self):\n",
    "                self.grad_x_val = -self.y_label / self.y_out\n",
    "                return self.grad_x_val\n",
    "\n",
    "\n",
    "        class softmax_cross_entropy:\n",
    "            def __init__(self, y_label, y_out):\n",
    "                self.y_label = y_label\n",
    "                self.y_out = y_out\n",
    "            \n",
    "            def out(self):\n",
    "                softmax = NN.Activations.softmax(self.y_out)\n",
    "                self.out_val = -np.sum(self.y_label * np.log(softmax.out()), axis = 1)\n",
    "                return self.out_val\n",
    "            \n",
    "            def grad_z(self):\n",
    "                self.grad_z_val = self.y_out - self.y_label\n",
    "                return self.grad_z_val\n",
    "    \n",
    "    \n",
    "    class Metrics:\n",
    "        def accuracy(y_label, y_out):\n",
    "            correct_prediction = np.equal(np.argmax(y_label, axis = 1), np.argmax(y_out, axis = 1))\n",
    "            return correct_prediction.astype(np.float32).mean()\n",
    "\n",
    "\n",
    "    class Optimizers:\n",
    "        class Adagrad:\n",
    "            def __init__(self, lr, n_layers, initial_accumulator_value=0.1):\n",
    "                self.lr = lr\n",
    "                self.dw_sum_sq = [initial_accumulator_value for i in range(n_layers)]\n",
    "                self.db_sum_sq = [initial_accumulator_value for i in range(n_layers)]\n",
    "\n",
    "            def step(self, dw, db):\n",
    "                for i in range(len(dw)):\n",
    "                    if not dw[i] is None:\n",
    "                        self.dw_sum_sq[i] += dw[i] ** 2\n",
    "                    if not db[i] is None:\n",
    "                        self.db_sum_sq[i] += db[i] ** 2\n",
    "\n",
    "            def lr_eff(self):\n",
    "                lr_eff_w = []\n",
    "                lr_eff_b = []\n",
    "                for i in range(len(self.dw_sum_sq)):\n",
    "                    # print((self.lr, np.sqrt(self.dw_sum_sq[i]).shape, self.lr / np.sqrt(self.dw_sum_sq[i])))\n",
    "                    lr_eff_w.append(self.lr / np.sqrt(self.dw_sum_sq[i]))\n",
    "                    lr_eff_b.append(self.lr / np.sqrt(self.db_sum_sq[i]))\n",
    "                return lr_eff_w, lr_eff_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, x_image_shape=[28, 28], lr=1e-3, optimizer=None, cost=NN.Cost.mean_squared_error):\n",
    "        self.started = False\n",
    "        self.epochs_completed = 0\n",
    "        self.x_image_shape = x_image_shape\n",
    "        self.lr = lr\n",
    "        self.optimizer_func = optimizer\n",
    "        self.cost_func = cost\n",
    "        self.w = []\n",
    "        self.dw = []\n",
    "        self.b = []\n",
    "        self.db = []\n",
    "        self.f = []\n",
    "        self.s = []\n",
    "\n",
    "    def optimize_with(self, optimizer_func, lr=None):\n",
    "        self.optimizer_func = optimizer_func\n",
    "        if lr != self.lr:\n",
    "            self.lr = lr\n",
    "        return self\n",
    "\n",
    "    def minimize(self, cost_func):\n",
    "        self.cost_func = cost_func\n",
    "        return self\n",
    "\n",
    "    def init_layer_vars(self, w=None, b=None):\n",
    "        if not self.started:\n",
    "            self.w.append(w)\n",
    "            self.dw.append(None)\n",
    "            self.b.append(b)\n",
    "            self.db.append(None)\n",
    "            self.f.append(None)\n",
    "            self.s.append(None)\n",
    "        \n",
    "    def model(self, batch_x_images):\n",
    "        # Layer 0 (2D convolution layer)\n",
    "        filter_shape_0 = [10, 10]\n",
    "        in_channels_0 = 1\n",
    "        out_channels_0 = 32\n",
    "\n",
    "        # Reshape input\n",
    "        x_0 = np.reshape(batch_x_images, [batch_x_images.shape[0]] + list(self.x_image_shape) + [in_channels_0])\n",
    "\n",
    "        self.init_layer_vars(w = NN.Variables.weight_variable(filter_shape_0 + [in_channels_0, out_channels_0]),\n",
    "                             b = NN.Variables.bias_variable([out_channels_0]))\n",
    "\n",
    "        self.f[0] = NN.Layers.conv2d(x_0, self.w[0])\n",
    "        z_0 = self.f[0].out() + self.b[0]\n",
    "        \n",
    "        self.s[0] = NN.Activations.relu(z_0)\n",
    "        x_1 = self.s[0].out()\n",
    "\n",
    "        # Layer 1 (Max pool 2x2)\n",
    "        in_channels_1 = out_channels_0\n",
    "        out_channels_1 = in_channels_1\n",
    "        \n",
    "        self.init_layer_vars()\n",
    "\n",
    "        self.f[1] = NN.Layers.maxpool2d(x_1)\n",
    "        x_2 = self.f[1].out()\n",
    "\n",
    "        # Layer 2 (2D convolution layer)\n",
    "        filter_shape_2 = [5, 5]\n",
    "        in_channels_2 = out_channels_1\n",
    "        out_channels_2 = 16\n",
    "\n",
    "        self.init_layer_vars(w = NN.Variables.weight_variable(filter_shape_2 + [in_channels_2, out_channels_2]),\n",
    "                             b = NN.Variables.bias_variable([out_channels_2]))\n",
    "\n",
    "        self.f[2] = NN.Layers.conv2d(x_2, self.w[2])\n",
    "        z_2 = self.f[2].out() + self.b[2]\n",
    "        \n",
    "        self.s[2] = NN.Activations.relu(z_2)\n",
    "        x_3 = self.s[2].out()\n",
    "\n",
    "        # Layer 3 (Max pool 2x2)\n",
    "        in_channels_3 = out_channels_2\n",
    "        out_channels_3 = in_channels_3\n",
    "        \n",
    "        self.init_layer_vars()\n",
    "\n",
    "        self.f[3] = NN.Layers.maxpool2d(x_3)\n",
    "        x_4 = self.f[3].out()\n",
    "\n",
    "        # Layer 4 (Flatten)\n",
    "        in_channels_4 = out_channels_3\n",
    "        out_channels_4 = in_channels_4\n",
    "        \n",
    "        self.init_layer_vars()\n",
    "\n",
    "        self.f[4] = NN.Layers.flatten(x_4)\n",
    "        x_5 = self.f[4].out()\n",
    "\n",
    "        # Layer 5 (Fully connected layer)\n",
    "        in_channels_5 = x_5.shape[-1]\n",
    "        out_channels_5 = 1024\n",
    "\n",
    "        self.init_layer_vars(w = NN.Variables.weight_variable([in_channels_5, out_channels_5]),\n",
    "                             b = NN.Variables.bias_variable([out_channels_5]))\n",
    "\n",
    "        self.f[5] = NN.Layers.fullconn(x_5, self.w[5])\n",
    "        z_5 = self.f[5].out() + self.b[5]\n",
    "\n",
    "        self.s[5] = NN.Activations.relu(z_5)\n",
    "        x_6 = self.s[5].out()\n",
    "\n",
    "        # Layer 6 (Fully connected layer)\n",
    "        in_channels_6 = out_channels_5\n",
    "        out_channels_6 = 10\n",
    "\n",
    "        self.init_layer_vars(w = NN.Variables.weight_variable([in_channels_6, out_channels_6]),\n",
    "                             b = NN.Variables.bias_variable([out_channels_6]))\n",
    "\n",
    "        self.f[6] = NN.Layers.fullconn(x_6, self.w[6])\n",
    "        z_6 = self.f[6].out() + self.b[6]\n",
    "\n",
    "        self.s[6] = NN.Activations.softmax(z_6)\n",
    "        x_7 = self.s[6].out()\n",
    "        \n",
    "        self.batch_y_out = x_7\n",
    "        \n",
    "        return self.batch_y_out\n",
    "    \n",
    "    def backpropagate(self, batch_y_labels):\n",
    "        n_layers = len(self.dw)\n",
    "        if not self.started:\n",
    "            if not self.optimizer_func is None:\n",
    "                self.optimizer = self.optimizer_func(self.lr, n_layers)\n",
    "            self.started = True\n",
    "\n",
    "        cost = self.cost_func(batch_y_labels, self.batch_y_out)\n",
    "\n",
    "        if self.optimizer_func is None:\n",
    "            lr_eff_w, lr_eff_b = [self.lr for i in range(n_layers)], [self.lr for i in range(n_layers)]\n",
    "        else:\n",
    "            lr_eff_w, lr_eff_b = self.optimizer.lr_eff()\n",
    "\n",
    "        if 'grad_z' in vars(self.cost_func):\n",
    "            delta = -cost.grad_z()\n",
    "        elif 'grad_x' in vars(self.cost_func):\n",
    "            delta = -self.s[n_layers - 1].grad_z(cost.grad_x())\n",
    "        else:\n",
    "            raise Exception(\"No cost function recognized.\")\n",
    "\n",
    "        if not self.w[n_layers - 1] is None:\n",
    "            # print(np.mean(np.sum(delta, axis = tuple(range(1, delta.ndim - 1))), axis = 0))\n",
    "            self.db[n_layers - 1] = lr_eff_b[n_layers - 1] * np.sum(delta, axis = tuple(range(delta.ndim - 1)))\n",
    "            self.b[n_layers - 1] += self.db[n_layers - 1]\n",
    "            # print(self.f[n_layers - 1].grad_w(delta).shape)\n",
    "            self.dw[n_layers - 1] = lr_eff_w[n_layers - 1] * self.f[n_layers - 1].grad_w(delta)\n",
    "            # print(\"{}: {}\".format(n_layers - 1, self.dw[n_layers - 1]))\n",
    "            self.w[n_layers - 1] += self.dw[n_layers - 1]\n",
    "        \n",
    "        for i in reversed(range(n_layers - 1)):\n",
    "            if self.w[i] is None:\n",
    "                delta = self.f[i + 1].grad_x(delta)\n",
    "            else:\n",
    "                delta = self.s[i].grad_z(self.f[i + 1].grad_x(delta))\n",
    "                # print(delta.shape)\n",
    "                self.db[i] = lr_eff_b[i] * np.sum(delta, axis = tuple(range(delta.ndim - 1)))\n",
    "                self.b[i] += self.db[i]\n",
    "                # print(self.f[i].grad_w(delta).shape)\n",
    "                self.dw[i] = lr_eff_w[i] * self.f[i].grad_w(delta)\n",
    "                # print(\"{}: {}\".format(i, self.dw[i]))\n",
    "                self.w[i] += self.dw[i]\n",
    "\n",
    "        if not self.optimizer_func is None:\n",
    "            self.optimizer.step(self.dw, self.db)\n",
    "\n",
    "    def train_step(self, batch_x_images, batch_y_labels, timing=False):\n",
    "        batch_size = batch_x_images.shape[0]\n",
    "        if timing:\n",
    "            start_forward_time = time()\n",
    "        batch_y_out = self.model(batch_x_images)\n",
    "        if timing:\n",
    "            forward_time = (time() - start_forward_time) / batch_size\n",
    "            start_backward_time = time()\n",
    "        self.backpropagate(batch_y_labels)\n",
    "        if timing:\n",
    "            backward_time = (time() - start_backward_time) / batch_size\n",
    "            return batch_y_out, forward_time, backward_time\n",
    "        return batch_y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/tensorflow/mnist/input_data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/tensorflow/mnist/input_data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/tensorflow/mnist/input_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/tensorflow/mnist/input_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Import data\n",
    "path = '/tmp/tensorflow/mnist/input_data'\n",
    "mnist = input_data.read_data_sets(path, one_hot=True)\n",
    "\n",
    "batch_x_images, batch_y_labels = mnist.train.next_batch(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "model = Model(lr = 1e-3).minimize(NN.Cost.softmax_cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2\n",
    "x_image_shape = [28, 28]\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "\n",
    "# Layer 0 (2D convolution layer)\n",
    "filter_shape_0 = [10, 10]\n",
    "in_channels_0 = 1\n",
    "out_channels_0 = 32\n",
    "\n",
    "# Reshape input\n",
    "batch = -1\n",
    "tf_x_0 = tf.reshape(x, [batch] + list(x_image_shape) + [in_channels_0])\n",
    "\n",
    "tf_w_0 = weight_variable(filter_shape_0 + [in_channels_0, out_channels_0])\n",
    "sess.run(tf_w_0.initializer)\n",
    "tf_b_0 = bias_variable([out_channels_0])\n",
    "sess.run(tf_b_0.initializer)\n",
    "\n",
    "tf_z_0 = conv2d(tf_x_0, tf_w_0) + tf_b_0\n",
    "\n",
    "tf_x_1 = tf.nn.relu(tf_z_0)\n",
    "\n",
    "# Layer 1 (Max pool 2x2)\n",
    "in_channels_1 = out_channels_0\n",
    "out_channels_1 = in_channels_1\n",
    "    \n",
    "tf_x_2 = maxpool2d(tf_x_1)\n",
    "\n",
    "# Layer 2 (2D convolution layer)\n",
    "filter_shape_2 = [5, 5]\n",
    "in_channels_2 = out_channels_1\n",
    "out_channels_2 = 16\n",
    "\n",
    "tf_w_2 = weight_variable(filter_shape_2 + [in_channels_2, out_channels_2])\n",
    "sess.run(tf_w_2.initializer)\n",
    "tf_b_2 = bias_variable([out_channels_2])\n",
    "sess.run(tf_b_2.initializer)\n",
    "\n",
    "tf_z_2 = conv2d(tf_x_2, tf_w_2) + tf_b_2\n",
    "\n",
    "tf_x_3 = tf.nn.relu(tf_z_2)\n",
    "\n",
    "# Layer 3 (Max pool 2x2)\n",
    "in_channels_3 = out_channels_2\n",
    "out_channels_3 = in_channels_3\n",
    "    \n",
    "tf_x_4 = maxpool2d(tf_x_3)\n",
    "\n",
    "# Layer 4 (Flatten)\n",
    "in_channels_4 = out_channels_3\n",
    "out_channels_4 = in_channels_4\n",
    "\n",
    "tf_x_5 = flatten(tf_x_4)\n",
    "\n",
    "# Layer 5 (Fully connected layer)\n",
    "in_channels_5 = tf_x_5.shape.as_list()[-1]\n",
    "out_channels_5 = 1024\n",
    "\n",
    "tf_w_5 = weight_variable([in_channels_5, out_channels_5])\n",
    "sess.run(tf_w_5.initializer)\n",
    "tf_b_5 = bias_variable([out_channels_5])\n",
    "sess.run(tf_b_5.initializer)\n",
    "\n",
    "tf_z_5 = fullconn(tf_x_5, tf_w_5) + tf_b_5\n",
    "\n",
    "tf_x_6 = tf.nn.relu(tf_z_5)\n",
    "\n",
    "# Layer 6 (Fully connected layer)\n",
    "in_channels_6 = out_channels_5\n",
    "out_channels_6 = 10\n",
    "\n",
    "tf_w_6 = weight_variable([in_channels_6, out_channels_6])\n",
    "sess.run(tf_w_6.initializer)\n",
    "tf_b_6 = bias_variable([out_channels_6])\n",
    "sess.run(tf_b_6.initializer)\n",
    "\n",
    "tf_z_6 = fullconn(tf_x_6, tf_w_6) + tf_b_6\n",
    "\n",
    "y = tf_z_6\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))\n",
    "train_step = tf.train.AdagradOptimizer(1e-3).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 3\n",
    "# Layer 0 (2D convolution layer)\n",
    "filter_shape_0 = [10, 10]\n",
    "in_channels_0 = 1\n",
    "out_channels_0 = 32\n",
    "\n",
    "# Reshape input\n",
    "x_0 = np.reshape(batch_x_images, [batch_x_images.shape[0]] + list(model.x_image_shape) + [in_channels_0])\n",
    "\n",
    "model.init_layer_vars(w = sess.run(tf_w_0),\n",
    "                     b = sess.run(tf_b_0))\n",
    "\n",
    "model.f[0] = NN.Layers.conv2d(x_0, model.w[0])\n",
    "z_0 = model.f[0].out() + model.b[0]\n",
    "\n",
    "model.s[0] = NN.Activations.relu(z_0)\n",
    "x_1 = model.s[0].out()\n",
    "\n",
    "# Layer 1 (Max pool 2x2)\n",
    "in_channels_1 = out_channels_0\n",
    "out_channels_1 = in_channels_1\n",
    "\n",
    "model.init_layer_vars()\n",
    "\n",
    "model.f[1] = NN.Layers.maxpool2d(x_1)\n",
    "x_2 = model.f[1].out()\n",
    "\n",
    "# Layer 2 (2D convolution layer)\n",
    "filter_shape_2 = [5, 5]\n",
    "in_channels_2 = out_channels_1\n",
    "out_channels_2 = 16\n",
    "\n",
    "model.init_layer_vars(w = sess.run(tf_w_2),\n",
    "                     b = sess.run(tf_b_2))\n",
    "\n",
    "model.f[2] = NN.Layers.conv2d(x_2, model.w[2])\n",
    "z_2 = model.f[2].out() + model.b[2]\n",
    "\n",
    "model.s[2] = NN.Activations.relu(z_2)\n",
    "x_3 = model.s[2].out()\n",
    "\n",
    "# Layer 3 (Max pool 2x2)\n",
    "in_channels_3 = out_channels_2\n",
    "out_channels_3 = in_channels_3\n",
    "\n",
    "model.init_layer_vars()\n",
    "\n",
    "model.f[3] = NN.Layers.maxpool2d(x_3)\n",
    "x_4 = model.f[3].out()\n",
    "\n",
    "# Layer 4 (Flatten)\n",
    "in_channels_4 = out_channels_3\n",
    "out_channels_4 = in_channels_4\n",
    "\n",
    "model.init_layer_vars()\n",
    "\n",
    "model.f[4] = NN.Layers.flatten(x_4)\n",
    "x_5 = model.f[4].out()\n",
    "\n",
    "# Layer 5 (Fully connected layer)\n",
    "in_channels_5 = x_5.shape[-1]\n",
    "out_channels_5 = 1024\n",
    "\n",
    "model.init_layer_vars(w = sess.run(tf_w_5),\n",
    "                     b = sess.run(tf_b_5))\n",
    "\n",
    "model.f[5] = NN.Layers.fullconn(x_5, model.w[5])\n",
    "z_5 = model.f[5].out() + model.b[5]\n",
    "\n",
    "model.s[5] = NN.Activations.relu(z_5)\n",
    "x_6 = model.s[5].out()\n",
    "\n",
    "# Layer 6 (Fully connected layer)\n",
    "in_channels_6 = out_channels_5\n",
    "out_channels_6 = 10\n",
    "\n",
    "model.init_layer_vars(w = sess.run(tf_w_6),\n",
    "                     b = sess.run(tf_b_6))\n",
    "\n",
    "model.f[6] = NN.Layers.fullconn(x_6, model.w[6])\n",
    "z_6 = model.f[6].out() + model.b[6]\n",
    "\n",
    "model.s[6] = NN.Activations.softmax(z_6)\n",
    "x_7 = model.s[6].out()\n",
    "\n",
    "model.batch_y_out = x_7\n",
    "\n",
    "model.started = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(model.b[0], sess.run(tf_b_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9.8347664e-07, 0.1)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_7 - sess.run(tf.nn.softmax(tf_z_6), feed_dict={x: batch_x_images, y_: batch_y_labels})).max(), x_7.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.00000001e-01,   1.00000001e-01,   1.00000001e-01,\n",
       "          1.00000001e-01,   1.00000001e-01,   1.00000001e-01,\n",
       "          1.00000001e-01,   1.00000001e-01,   1.07168444e-01,\n",
       "          1.24769822e-01,   1.71549857e-01,   2.65877604e-01,\n",
       "          3.21641982e-01,   3.85279626e-01,   3.76678199e-01,\n",
       "          2.55690515e-01,   2.10916668e-01,   1.84303433e-01,\n",
       "         -9.55344662e-02,  -2.13925153e-01,  -9.28050503e-02,\n",
       "         -1.93637982e-02,  -2.47329846e-02,  -3.67036089e-02,\n",
       "          7.17024803e-02,   1.00000001e-01,   1.00000001e-01,\n",
       "          1.00000001e-01],\n",
       "       [  1.00000001e-01,   1.00000001e-01,   1.00000001e-01,\n",
       "          1.00000001e-01,   1.00000001e-01,   1.00000001e-01,\n",
       "          1.00000001e-01,   1.19937226e-01,   1.91691026e-01,\n",
       "          2.49636263e-01,   3.30932796e-01,   3.54055703e-01,\n",
       "          2.44973123e-01,   2.13747621e-01,   2.38689244e-01,\n",
       "          1.75958440e-01,   2.21730098e-02,   6.51277602e-03,\n",
       "          9.32156146e-02,  -6.11844733e-02,  -1.09063663e-01,\n",
       "         -1.33401841e-01,  -1.07680850e-01,   2.17106864e-02,\n",
       "          4.80739847e-02,   1.00000001e-01,   1.00000001e-01,\n",
       "          1.00000001e-01],\n",
       "       [  1.00000001e-01,   1.00000001e-01,   1.00000001e-01,\n",
       "          1.00000001e-01,   1.00000001e-01,   1.00000001e-01,\n",
       "          1.00000001e-01,   1.56021893e-01,   2.52905786e-01,\n",
       "          2.95182675e-01,   3.32503259e-01,   2.91617095e-01,\n",
       "          1.46228880e-01,   1.55422077e-01,   8.44042152e-02,\n",
       "          1.02332309e-01,   2.18254894e-01,   1.90574080e-01,\n",
       "          2.80906558e-02,   4.78474982e-02,   7.47439489e-02,\n",
       "          1.15122095e-01,  -2.11819336e-02,  -8.04580823e-02,\n",
       "          1.88741088e-03,   7.06283748e-02,   1.00000001e-01,\n",
       "          1.00000001e-01],\n",
       "       [  1.00000001e-01,   1.00000001e-01,   1.00000001e-01,\n",
       "          1.00000001e-01,   1.00000001e-01,   1.00000001e-01,\n",
       "          1.21953353e-01,   1.91034913e-01,   2.02442735e-01,\n",
       "          2.46036649e-01,   2.10534781e-01,   6.01963438e-02,\n",
       "          9.00457799e-02,   2.98286319e-01,   3.67303967e-01,\n",
       "          2.91405290e-01,  -2.35211849e-02,   2.46887431e-02,\n",
       "          2.34027952e-03,   1.50702894e-01,   1.02665707e-01,\n",
       "         -8.34930465e-02,  -4.23279032e-02,  -6.26092777e-02,\n",
       "         -1.50667578e-02,   2.70197988e-02,   1.00000001e-01,\n",
       "          1.00000001e-01],\n",
       "       [  1.00000001e-01,   1.00000001e-01,   1.00000001e-01,\n",
       "          1.00000001e-01,   1.00000001e-01,   1.08512528e-01,\n",
       "          1.88150793e-01,   2.37299651e-01,   2.81307459e-01,\n",
       "          3.63435239e-01,   1.86202914e-01,   1.43323600e-01,\n",
       "          2.81620711e-01,   1.64727762e-01,   1.91529721e-01,\n",
       "          2.37214744e-01,   1.62606642e-01,  -2.27861032e-02,\n",
       "          1.36260748e-01,   1.32852942e-01,   1.80596337e-02,\n",
       "          1.45342499e-02,  -8.60108361e-02,  -1.44114643e-01,\n",
       "         -5.78285679e-02,   5.09235486e-02,   1.00000001e-01,\n",
       "          1.00000001e-01],\n",
       "       [  1.00000001e-01,   1.00000001e-01,   1.00000001e-01,\n",
       "          1.00000001e-01,   1.02240138e-01,   1.39633134e-01,\n",
       "          2.10774451e-01,   2.37956464e-01,   3.95296156e-01,\n",
       "          3.60063046e-01,   2.33664751e-01,   2.22110927e-01,\n",
       "          2.55548447e-01,   1.53510615e-01,   4.19701375e-02,\n",
       "         -1.88370273e-02,   1.12843148e-01,   2.89825469e-01,\n",
       "          1.34001374e-01,   7.26205185e-02,   1.56130761e-01,\n",
       "          8.78277048e-02,  -3.29776928e-02,  -1.94396257e-01,\n",
       "         -7.85111263e-02,   3.36244032e-02,   8.38744044e-02,\n",
       "          1.00000001e-01],\n",
       "       [  1.00000001e-01,   1.00000001e-01,   1.00000001e-01,\n",
       "          1.00000001e-01,   1.08688273e-01,   1.77855074e-01,\n",
       "          2.16305912e-01,   2.66442448e-01,   3.08844447e-01,\n",
       "          1.13503717e-01,   1.50903970e-01,   2.02058971e-01,\n",
       "          2.64602393e-01,  -7.61158764e-04,   9.75233018e-02,\n",
       "          3.56182516e-01,   4.70947653e-01,   6.79189622e-01,\n",
       "          4.32605952e-01,   3.12929600e-01,   3.32822204e-01,\n",
       "          4.18382347e-01,   9.50828940e-02,  -1.55085862e-01,\n",
       "         -1.04530655e-01,   5.28593138e-02,   1.10398330e-01,\n",
       "          1.00000001e-01],\n",
       "       [  1.00000001e-01,   1.00000001e-01,   1.00000001e-01,\n",
       "          1.00000001e-01,   1.33659080e-01,   2.16012597e-01,\n",
       "          2.43635416e-01,   4.02222902e-01,   2.66654342e-01,\n",
       "          2.05191761e-01,   3.56528461e-02,   1.13528475e-01,\n",
       "          1.35766700e-01,   1.20839596e-01,   2.86987156e-01,\n",
       "          4.81941253e-01,   7.47735322e-01,   8.22861612e-01,\n",
       "          7.26027071e-01,   7.60916650e-01,   5.71393013e-01,\n",
       "          6.00795925e-01,   3.21842730e-01,  -5.98627552e-02,\n",
       "         -1.17135160e-01,   1.29753575e-02,   9.54351202e-02,\n",
       "          1.00000001e-01],\n",
       "       [  1.00000001e-01,   1.00000001e-01,   1.00000001e-01,\n",
       "          1.00000001e-01,   1.56904399e-01,   1.87792152e-01,\n",
       "          2.70303667e-01,   2.82182902e-01,   2.83606559e-01,\n",
       "         -1.35619491e-02,   1.53807253e-01,   3.65225881e-01,\n",
       "          4.85952973e-01,   5.03681540e-01,   4.90363806e-01,\n",
       "          9.66372967e-01,   1.03246939e+00,   8.76160920e-01,\n",
       "          7.21961677e-01,   9.54304993e-01,   6.53849483e-01,\n",
       "          4.76226419e-01,   3.84720415e-01,   1.97436839e-01,\n",
       "         -3.98183689e-02,   1.64036900e-02,   1.05201297e-01,\n",
       "          1.00000001e-01],\n",
       "       [  1.00000001e-01,   1.00000001e-01,   1.00000001e-01,\n",
       "          1.19937226e-01,   1.71515614e-01,   2.37228543e-01,\n",
       "          2.75785565e-01,   2.33516842e-01,  -2.87031606e-02,\n",
       "         -7.34398291e-02,   3.41294289e-01,   5.12340784e-01,\n",
       "          5.35975337e-01,   6.17012084e-01,   7.91463792e-01,\n",
       "          9.76775527e-01,   9.18115497e-01,   7.96758950e-01,\n",
       "          7.70191014e-01,   7.96574831e-01,   5.31768739e-01,\n",
       "          3.56079340e-01,   1.75909072e-01,   3.01116794e-01,\n",
       "          1.54603854e-01,   1.08321883e-01,   9.25694555e-02,\n",
       "          1.00000001e-01],\n",
       "       [  1.00000001e-01,   1.00000001e-01,   1.08512528e-01,\n",
       "          1.84161991e-01,   2.42096692e-01,   2.98392504e-01,\n",
       "          4.15190101e-01,   2.31701076e-01,  -3.22115496e-02,\n",
       "          2.54503459e-01,   6.34371400e-01,   6.53554559e-01,\n",
       "          6.50365353e-01,   6.69789374e-01,   7.17551231e-01,\n",
       "          8.06765616e-01,   8.12887788e-01,   6.54682159e-01,\n",
       "          6.24731541e-01,   6.41364813e-01,   4.25900489e-01,\n",
       "          4.48179580e-02,   1.74526259e-01,   4.22444135e-01,\n",
       "          3.94912899e-01,   9.45501179e-02,   9.46919024e-02,\n",
       "          1.00000001e-01],\n",
       "       [  1.00000001e-01,   1.00000001e-01,   1.41259134e-01,\n",
       "          2.22408772e-01,   2.35749811e-01,   3.56333435e-01,\n",
       "          2.50796497e-01,  -1.30950138e-02,   7.51064941e-02,\n",
       "          5.05132616e-01,   7.72332430e-01,   6.86201274e-01,\n",
       "          6.26434684e-01,   5.41390836e-01,   7.77616203e-01,\n",
       "          6.38735950e-01,   7.16820180e-01,   5.49261928e-01,\n",
       "          7.37622738e-01,   4.99826252e-01,   3.03597003e-01,\n",
       "         -1.41288638e-01,  -2.19907835e-02,   4.45844889e-01,\n",
       "          4.43083555e-01,   2.16396272e-01,   1.01978786e-01,\n",
       "          1.00000001e-01],\n",
       "       [  1.00000001e-01,   1.00000001e-01,   1.60074741e-01,\n",
       "          1.96847722e-01,   2.26532012e-01,   2.83050925e-01,\n",
       "          1.28044143e-01,  -9.38614532e-02,   2.63733000e-01,\n",
       "          7.83671856e-01,   8.36783290e-01,   6.96106374e-01,\n",
       "          4.95043218e-01,   5.98261714e-01,   7.32963264e-01,\n",
       "          5.14534950e-01,   6.81625366e-01,   4.87307966e-01,\n",
       "          7.56334484e-01,   5.26543140e-01,   1.93985879e-01,\n",
       "         -7.95276389e-02,   2.47221142e-02,   5.57217956e-01,\n",
       "          4.15319175e-01,   2.36128420e-01,   1.06609337e-01,\n",
       "          1.00000001e-01],\n",
       "       [  1.00000001e-01,   1.00000001e-01,   1.29036471e-01,\n",
       "          2.22826749e-01,   2.76145428e-01,   2.66668350e-01,\n",
       "          9.99745205e-02,   8.46269205e-02,   5.29590309e-01,\n",
       "          9.63223398e-01,   6.56809390e-01,   3.83095711e-01,\n",
       "          2.88733691e-01,   5.07007897e-01,   5.09987950e-01,\n",
       "          4.79006439e-01,   4.09407288e-01,   4.75957900e-01,\n",
       "          6.70328379e-01,   5.96439362e-01,   2.18147606e-01,\n",
       "          4.99135219e-02,   1.87724143e-01,   5.14101505e-01,\n",
       "          3.77740622e-01,   2.90155381e-01,   1.15481816e-01,\n",
       "          1.00000001e-01],\n",
       "       [  1.00000001e-01,   1.00000001e-01,   1.08774483e-01,\n",
       "          2.11400971e-01,   2.75428295e-01,   1.81060642e-01,\n",
       "          1.96526498e-02,   3.41951430e-01,   7.76903927e-01,\n",
       "          8.34081173e-01,   4.47918445e-01,   2.08916396e-01,\n",
       "          2.58492738e-01,   2.81908542e-01,   3.95271182e-01,\n",
       "          3.55638295e-01,   4.70779896e-01,   7.18546212e-01,\n",
       "          7.45730698e-01,   5.73580325e-01,   2.28280783e-01,\n",
       "          3.11241925e-01,   2.15846807e-01,   5.13712525e-01,\n",
       "          4.02295023e-01,   2.84327358e-01,   1.06951796e-01,\n",
       "          1.00000001e-01],\n",
       "       [  1.00000001e-01,   1.00000001e-01,   8.34125727e-02,\n",
       "          1.87175140e-01,   2.75484741e-01,   1.17202446e-01,\n",
       "          1.44553512e-01,   6.73901916e-01,   9.77560818e-01,\n",
       "          7.90575206e-01,   4.31226522e-01,   2.64680088e-01,\n",
       "          3.25755298e-01,   2.75935233e-01,   2.69414604e-01,\n",
       "          5.00591770e-02,   9.70511958e-02,   3.69031936e-01,\n",
       "          5.54819226e-01,   3.45090687e-01,  -3.49938199e-02,\n",
       "          2.08868429e-01,   2.60517776e-01,   5.02165616e-01,\n",
       "          3.61940116e-01,   2.70250022e-01,   1.00826263e-01,\n",
       "          1.00000001e-01],\n",
       "       [  1.00000001e-01,   1.00000001e-01,   1.21400788e-01,\n",
       "          3.33593428e-01,   3.41911077e-01,   2.29295015e-01,\n",
       "          4.96667683e-01,   9.72843885e-01,   1.03750479e+00,\n",
       "          8.51002157e-01,   4.98968542e-01,   3.40631843e-01,\n",
       "          4.56020474e-01,   4.73958045e-01,   1.65695876e-01,\n",
       "          9.00292546e-02,   2.38128573e-01,   5.47231734e-01,\n",
       "          5.68378091e-01,   3.03370297e-01,   1.93660408e-01,\n",
       "          3.90310377e-01,   3.63253117e-01,   5.04139483e-01,\n",
       "          2.48274714e-01,   2.23251164e-01,   1.00000001e-01,\n",
       "          1.00000001e-01],\n",
       "       [  1.00000001e-01,   1.00000001e-01,   1.95959032e-01,\n",
       "          2.75561094e-01,   2.81172603e-01,   2.69690454e-01,\n",
       "          6.84178710e-01,   9.41578269e-01,   6.39748812e-01,\n",
       "          4.43338811e-01,   2.24376082e-01,   1.79056793e-01,\n",
       "          2.02054307e-01,   7.41126239e-02,   3.20876017e-02,\n",
       "          2.28289783e-01,   5.78385532e-01,   6.93217039e-01,\n",
       "          5.27526736e-01,   2.69711733e-01,   2.51216799e-01,\n",
       "          2.42577910e-01,   4.34580654e-01,   4.02948469e-01,\n",
       "          1.94235116e-01,   1.30531400e-01,   1.00000001e-01,\n",
       "          1.00000001e-01],\n",
       "       [  1.00000001e-01,   1.00000001e-01,   9.25155133e-02,\n",
       "         -9.08864290e-03,   5.92368431e-02,   2.60370046e-01,\n",
       "          6.33515716e-01,   9.28659856e-01,   7.15855777e-01,\n",
       "          2.76251346e-01,   2.17404068e-01,   3.08462501e-01,\n",
       "          3.57348144e-01,   8.47703218e-03,   1.63046658e-01,\n",
       "          5.98878145e-01,   9.57124054e-01,   9.37965274e-01,\n",
       "          6.60823941e-01,   3.90605062e-01,   4.62587357e-01,\n",
       "          4.01116490e-01,   5.07839620e-01,   2.67631084e-01,\n",
       "          2.00401723e-01,   1.02295175e-01,   1.00000001e-01,\n",
       "          1.00000001e-01],\n",
       "       [  1.00000001e-01,   1.00000001e-01,   1.07696295e-01,\n",
       "          2.00536400e-02,   1.28655478e-01,   3.40595365e-01,\n",
       "          6.99416161e-01,   7.62624979e-01,   5.64257264e-01,\n",
       "          3.93107712e-01,   5.70703328e-01,   6.36831582e-01,\n",
       "          4.17288125e-01,   3.38522494e-01,   4.56997216e-01,\n",
       "          6.47202492e-01,   5.29114425e-01,   3.05161417e-01,\n",
       "          2.19229966e-01,   2.82945752e-01,   3.74661058e-01,\n",
       "          4.25933093e-01,   3.33469003e-01,   1.99009240e-01,\n",
       "          1.50372833e-01,   1.00000001e-01,   1.00000001e-01,\n",
       "          1.00000001e-01],\n",
       "       [  1.00000001e-01,   1.00000001e-01,   1.60861671e-01,\n",
       "          1.98826998e-01,   4.28997397e-01,   5.23948133e-01,\n",
       "          4.64390516e-01,   5.11423886e-01,   1.00394063e-01,\n",
       "         -4.08273414e-02,   2.84915477e-01,   4.75498110e-01,\n",
       "          3.23677212e-01,   2.04905897e-01,   3.86974871e-01,\n",
       "          6.20244026e-01,   3.74832034e-01,   2.91555375e-01,\n",
       "          3.04200083e-01,   2.32503355e-01,   2.32477695e-01,\n",
       "          3.00500005e-01,   1.79796875e-01,   1.26824737e-01,\n",
       "          1.06885530e-01,   1.00000001e-01,   1.00000001e-01,\n",
       "          1.00000001e-01],\n",
       "       [  1.00000001e-01,   1.00000001e-01,   8.04287046e-02,\n",
       "          6.40747324e-02,   3.75797570e-01,   5.97502232e-01,\n",
       "          4.55002815e-01,   5.10500371e-01,   4.71732318e-01,\n",
       "          2.64480054e-01,   2.30038017e-01,   2.98895448e-01,\n",
       "          3.88614744e-01,   4.56729203e-01,   5.34765065e-01,\n",
       "          6.31878078e-01,   2.99522012e-01,   9.38442722e-02,\n",
       "          1.31387830e-01,   1.30367622e-01,   1.82635754e-01,\n",
       "          1.36165351e-01,   1.47510111e-01,   1.07436374e-01,\n",
       "          1.00000001e-01,   1.00000001e-01,   1.00000001e-01,\n",
       "          1.00000001e-01],\n",
       "       [  1.00000001e-01,   1.00000001e-01,   1.75580978e-01,\n",
       "          1.87130153e-01,   1.53204620e-01,   2.42309004e-01,\n",
       "          3.75154197e-01,   4.84155148e-01,   3.66561502e-01,\n",
       "          2.99176663e-01,   3.45855772e-01,   5.37725568e-01,\n",
       "          8.35995972e-01,   9.24419403e-01,   8.55421066e-01,\n",
       "          7.95983732e-01,   5.61178803e-01,   3.81322503e-01,\n",
       "          3.48207831e-01,   3.57483238e-01,   2.49849319e-01,\n",
       "          8.17823634e-02,   1.11934923e-01,   1.00000001e-01,\n",
       "          1.00000001e-01,   1.00000001e-01,   1.00000001e-01,\n",
       "          1.00000001e-01],\n",
       "       [  1.00000001e-01,   1.00000001e-01,   1.65089548e-01,\n",
       "          2.08409533e-01,   1.01769418e-01,   7.69295022e-02,\n",
       "          1.12695314e-01,   2.74796098e-01,   3.14582020e-01,\n",
       "          4.18084264e-01,   6.91823840e-01,   8.97134602e-01,\n",
       "          1.05072904e+00,   1.16163743e+00,   9.36811805e-01,\n",
       "          7.33425498e-01,   6.28283143e-01,   6.34909093e-01,\n",
       "          5.78241944e-01,   4.70753729e-01,   3.12534094e-01,\n",
       "          1.63312301e-01,   1.00000001e-01,   1.00000001e-01,\n",
       "          1.00000001e-01,   1.00000001e-01,   1.00000001e-01,\n",
       "          1.00000001e-01],\n",
       "       [  1.00000001e-01,   1.00000001e-01,   5.34440167e-02,\n",
       "          2.15950012e-02,   1.45606324e-01,   3.14787328e-01,\n",
       "          3.42305928e-01,   4.80625242e-01,   6.18524373e-01,\n",
       "          6.77382290e-01,   8.34214687e-01,   1.10744774e+00,\n",
       "          1.08277512e+00,   9.61593509e-01,   7.77361214e-01,\n",
       "          6.03221297e-01,   4.40374255e-01,   3.35930824e-01,\n",
       "          3.29309314e-01,   2.35090643e-01,   1.68086320e-01,\n",
       "          1.28964394e-01,   1.00000001e-01,   1.00000001e-01,\n",
       "          1.00000001e-01,   1.00000001e-01,   1.00000001e-01,\n",
       "          1.00000001e-01],\n",
       "       [  1.00000001e-01,   1.00000001e-01,   1.30518019e-01,\n",
       "          1.29065990e-01,   1.48930311e-01,   3.29103321e-01,\n",
       "          4.33343172e-01,   4.91585374e-01,   6.05844855e-01,\n",
       "          6.31510437e-01,   6.06280923e-01,   5.88364184e-01,\n",
       "          6.67173862e-01,   6.14395440e-01,   5.10022938e-01,\n",
       "          3.60969692e-01,   1.77831098e-01,   1.42927006e-01,\n",
       "          1.29749566e-01,   1.15963414e-01,   1.13603778e-01,\n",
       "          1.03488669e-01,   1.00000001e-01,   1.00000001e-01,\n",
       "          1.00000001e-01,   1.00000001e-01,   1.00000001e-01,\n",
       "          1.00000001e-01],\n",
       "       [  1.00000001e-01,   1.00000001e-01,   1.22256376e-01,\n",
       "          1.82966501e-01,   1.84664845e-01,   2.38511592e-01,\n",
       "          3.73423100e-01,   4.05628741e-01,   4.47076231e-01,\n",
       "          3.63749593e-01,   3.15719455e-01,   2.27506936e-01,\n",
       "          2.13692069e-01,   2.00754702e-01,   1.44640833e-01,\n",
       "          1.23328671e-01,   5.42893335e-02,   6.62959144e-02,\n",
       "          6.09856360e-02,   9.74653810e-02,   1.02846019e-01,\n",
       "          1.00000001e-01,   1.00000001e-01,   1.00000001e-01,\n",
       "          1.00000001e-01,   1.00000001e-01,   1.00000001e-01,\n",
       "          1.00000001e-01],\n",
       "       [  1.00000001e-01,   1.00000001e-01,   1.00000001e-01,\n",
       "          1.29284710e-01,   2.01334357e-01,   2.76825279e-01,\n",
       "          3.63923967e-01,   4.04771358e-01,   3.53043258e-01,\n",
       "          2.46630520e-01,   1.03923038e-01,  -3.34094465e-03,\n",
       "         -6.16114214e-02,  -2.52884254e-02,   2.29828656e-02,\n",
       "          5.76875880e-02,   1.02193885e-01,   1.02295175e-01,\n",
       "          1.00000001e-01,   1.00000001e-01,   1.00000001e-01,\n",
       "          1.00000001e-01,   1.00000001e-01,   1.00000001e-01,\n",
       "          1.00000001e-01,   1.00000001e-01,   1.00000001e-01,\n",
       "          1.00000001e-01]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(tf_z_0, feed_dict={x: batch_x_images, y_: batch_y_labels})[0, :, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(train_step, feed_dict={x: batch_x_images, y_: batch_y_labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Model' object has no attribute 'batch_y_out'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-b46618c8e8a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackpropagate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_y_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-66826657a3c8>\u001b[0m in \u001b[0;36mbackpropagate\u001b[0;34m(self, batch_y_labels)\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstarted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcost_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_y_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_y_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_func\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Model' object has no attribute 'batch_y_out'"
     ]
    }
   ],
   "source": [
    "model.backpropagate(batch_y_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
